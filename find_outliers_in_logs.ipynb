{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to find and group outliers in a logstream, based on behaviour\n",
    "\n",
    "## Context\n",
    "\n",
    "Your boss asked you to find suspect users among the few thousands of customers using your web site every day.\n",
    "\n",
    "Where do you start?! Easy! AI, Tensorflow, machine learning, and other buzzwords like so will solve the issue! When all logs are categorized, it's quite easy right?! You know what is going on in your logs right?\n",
    "\n",
    "... most probably not, especially if your site is generating few gigabytes of logs per day, there is just too much logs to know everything that is going one.\n",
    "\n",
    "Before even starting to search for suspicious users, step 1 would be starting by identifying what it means to be suspicious.\n",
    "\n",
    "    Suspicious normally means that it stands out from the normal.\n",
    "    \n",
    "Ok, step 2: what is normal?\n",
    "\n",
    "    Normal behaviour consist of expected usage. Anything outside of this can be considered abnormal.\n",
    "\n",
    "... We are not much more advanced because now we need to go over each action, and identify what is normal, and what is not, and assign a score to each.\n",
    "\n",
    "The second issue with this is that normal behaviour will be definition be way more likely than the abnormal one, which will skew the results into making any machine learning model think that 99.9% of the usage is normal, which a margin of error of + or - 5%, flooding your analysis with noise.\n",
    "\n",
    "The classical approach is to use a simple rule based system. By setting few triggers, we can find out account attackers. For small scale or simple systems, this will is likely to be enough, but not for large scale and complex systems.\n",
    "\n",
    "One other hypothesis would be to use Hidden Markov Chains, but for HNN, we need to know what we are searching for.\n",
    "Logs are also noisy: users will not simply go from A to B then C. they might crawl the whole site before reaching C, which complicates the creation of HMM.\n",
    "\n",
    "Another hypothesis would be to filter out known noise, like when we want to do sentiment analysis, English stop words are just too common and does not tell us anything useful... but what if these stop words are useless in small quantities, but an indicator of an issue in large quantities? ... and how much is too much?\n",
    "\n",
    "\n",
    "## Information Theory To The Rescue\n",
    "\n",
    "Information Theory is normally used for signal analysis and compression. By using Surprisal Analysis and Kullback–Leibler divergence, we can group similar behaviour togethers.\n",
    "\n",
    "* [Information Content](https://en.wikipedia.org/wiki/Information_content)\n",
    "* [Surprisal Analysis](https://en.wikipedia.org/wiki/Surprisal_analysis)\n",
    "* [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "* [Example of Surprisal](http://www.umsl.edu/~fraundorfp/egsurpri.html)\n",
    "\n",
    "Basically, Surprisal Analysis is a way to measure (in bits) how surprised we will be by looking at something. Surprisal are calculated using Log base 2, which makes it easy to calculate because log are additive when probabilities are multiplied.\n",
    "\n",
    "For example:\n",
    "* There is 1/6 chance to roll a 6 die, gives us 2.58 bits of information (-log2(1/6) ~= 2.4849 bits). If you are playing D&D, you will be happily surprised.\n",
    "* Not rolling a 6 on a die represents 0.263 bits (-log2(5/6) ~= 0.263 bits). So rolling anything but a 6 in that same D&D game will most likely leave you dissapointed. \n",
    "* The chance of rolling 10 x 6 in a row on a die is 1/60466176 (10 * -log2(1/6) ~= 25.85 bits). This is highly unlikely.\n",
    "\n",
    "By assigning a surprisal value to each action taken over all actions taken by all users gives us an idea how much surprisal by seeing any of them happening, and by adding the score of all actions of a user, we can get an idea how \"normal\" was all his/her actions.\n",
    "\n",
    "This is what this notebook intend to demonstrate: Using Surprisal analysis, we will assign a score to actions, and by adding up the score of each action, identify series of actions that are unlikely to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But first, we need some logs\n",
    "\n",
    "What is explained in this notebook can be applied to real logs, but for the experimentation, I added a \"blackbox\" library that will help to generate our users and the logs of these users based on a determined probability distribution.\n",
    "\n",
    "## Caviat: I had to cheat to achieve the desired results\n",
    "\n",
    "Because I am generating the logs through a known distribution, I am also able to know the type of user of each user, which makes it easy later on to debug and know how wrong I am.\n",
    "\n",
    "This cheat was also necessary to get a probability distribution of the classification function of this notebook. This probability distribution is likely to be different for live environment, and should be tweeked.\n",
    "\n",
    "| | Positive | Negative |\n",
    "| -- | -- | -- |\n",
    "| True | 0.875 | 0.102 |\n",
    "| False | 0.125 | 0.898 |\n",
    "\n",
    "I am using this probability distribution, along with the Bayes Theorem to update my belief that each tested candidates truely belong in the tested behaviour classification.\n",
    "\n",
    "## Different User Profiles Generated By The Library\n",
    "\n",
    "The following profiles are generated by the library.\n",
    "\n",
    "Normal users\n",
    "* Buyer\n",
    "* Merchants\n",
    "\n",
    "Abnormal users:\n",
    "* Scraper bots\n",
    "* Spammers\n",
    "* fraudster\n",
    "* Account Attackers\n",
    "\n",
    "Buyers and merchants represent 98% of our logs. Leaving 2% to the abnormal users. However, the actions taken by each users being on a probability distribution, it is possible to see an \"attakcer\" user being classified as a user, because that attacked might have had a change of heart and didn't attack after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This cell initialize the libraries needed for this notebook, \n",
    "## along with the blackbox functions used to generate the users and logs\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from math import log, pow\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from blackbox import distribution\n",
    "from blackbox import generate_userlist, generate_logs, cheat_calculate_hit_rate, cheat_lookup_all_users\n",
    "\n",
    "magic = distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating The User Database\n",
    "\n",
    "The following cell generates our user database.\n",
    "\n",
    "You will notice in this notebook that we are reinitializing the random seed quite often, just to keep consistence during the testing. Set the *random_seed* to False to randomize everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 ['merchant', 'buyer', 'buyer', 'buyer', 'merchant']\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "\n",
    "if random_seed:\n",
    "    random.seed(random_seed)\n",
    "\n",
    "## We define how many users here\n",
    "number_of_new_users = 2000\n",
    "\n",
    "\n",
    "existing_users = [] # Later on, we can add users to our list by supplying it to the generate_userlist function\n",
    "user_lists = generate_userlist(number_of_new_users, existing_users)\n",
    "\n",
    "print(len(user_lists), user_lists[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Logs For Day 1\n",
    "\n",
    "Note: The more users we have, the more log events will be generated. The probability distribution of each user ensures that they will start with a defined action, crawl the site following a defined pattern, and logout eventually, until the end of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44854 logs event generated for 2000 users\n",
      "CPU times: user 8.58 s, sys: 87.7 ms, total: 8.67 s\n",
      "Wall time: 8.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if random_seed:\n",
    "    random.seed(random_seed)\n",
    "\n",
    "start_time = datetime(2019,1,1,0,0)\n",
    "day1_logs = generate_logs(user_lists, start_time)\n",
    "\n",
    "print(len(day1_logs), 'logs event generated for', len(user_lists), 'users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the logs in a pandas dataframe\n",
    "\n",
    "The transition surprisal lookup table used in this notebook calculates scores based on the movements of the users between each actions. For example:\n",
    "\n",
    "* login (success) -> view_items (success) will result in a low surpisal value\n",
    "* login (fail) -> buy_item (success) never happened. If this sequence happen, this should be a huge red flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time           user   path status  uidx   realtype  \\\n",
      "466   2019-01-01 00:41:30  fraudster1829  login   fail  1829  fraudster   \n",
      "1035  2019-01-01 01:18:25   merchant1112  login   fail  1112   merchant   \n",
      "1227  2019-01-01 01:31:22       buyer413  login   fail   413      buyer   \n",
      "1291  2019-01-01 01:35:26      buyer1754  login   fail  1754      buyer   \n",
      "2177  2019-01-01 02:20:18       buyer609  login   fail   609      buyer   \n",
      "\n",
      "     prev_path prev_status  \n",
      "466                         \n",
      "1035                        \n",
      "1227                        \n",
      "1291                        \n",
      "2177                        \n"
     ]
    }
   ],
   "source": [
    "def transform_logs_to_pandas(logs):\n",
    "    data = pd.DataFrame(np.array(logs), columns=['time', 'user', 'path', 'status', 'uidx', 'realtype'])\n",
    "    \n",
    "    data['prev_path'] = data.groupby(['user'])['path'].shift(1)\n",
    "    data['prev_path'] = data['prev_path'].fillna(\"\")\n",
    "\n",
    "    data['prev_status'] = data.groupby(['user'])['status'].shift(1)\n",
    "    data['prev_status'] = data['prev_status'].fillna(\"\")\n",
    "    return data\n",
    "    \n",
    "day1_data = transform_logs_to_pandas(day1_logs)\n",
    "\n",
    "print(day1_data.loc[(day1_data['path'] == 'login') & (day1_data['status'] == 'fail')].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell generates the transition surprisal lookup table used to score each actions taken by the users.\n",
    "\n",
    "The format is as follow:\n",
    "\n",
    "```\n",
    "['current path'],['previous path']: {\n",
    "    'fail': 0, # How many time this action transition failed. (ex. View Items: Success, from: Login: Success)\n",
    "    'success': 13, # How many time this action transition succeded (ex. View Items: Fail, from: Login: Success)\n",
    "    'fsurprisal': 11.266786540694902, # Surprisal value if there is a failure happens\n",
    "    'ssurprisal': 7.56634682255381 # Surprisal value if that action is successful.\n",
    "    }\n",
    "```\n",
    "\n",
    "The surprisal value is directly related to the likelihood of an actions happening. If an actions is observed successfully few million times, then the successful surprisal value will be really low. However, the failure surprisal will be much higher if it never happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_transition_surprisal_lookup(data, key, prev_key, feature, success):\n",
    "    surprisal = {}\n",
    "\n",
    "    for pkey in data[key].unique():\n",
    "        data_for_pkey = data.loc[(data[key] == pkey)]\n",
    "        denum = len(data.loc[(data[key] == pkey)])\n",
    "\n",
    "        for ppkey in data_for_pkey[prev_key].unique():\n",
    "            ds = data_for_pkey.loc[(data_for_pkey[prev_key] == ppkey) & (data_for_pkey[feature] == success)]\n",
    "            df = data_for_pkey.loc[(data_for_pkey[prev_key] == ppkey) & (data_for_pkey[feature] != success)]\n",
    "\n",
    "            dsuccess = len(ds) * 1.0\n",
    "            dfail = len(df) * 1.0\n",
    "\n",
    "            if dsuccess == 0:\n",
    "                dsuccess = 1.0 \n",
    "\n",
    "            if dfail == 0:\n",
    "                dfail = 1.0\n",
    "\n",
    "            if (pkey not in surprisal.keys()):\n",
    "                surprisal[pkey] = {}\n",
    "\n",
    "            surprisal[pkey][ppkey] = {\n",
    "                'success': len(ds), \n",
    "                'fail': len(df), \n",
    "                'ssurprisal': log(1/(dsuccess / denum),2), \n",
    "                'fsurprisal': log(1/(dfail / denum),2),\n",
    "            }\n",
    "    return surprisal\n",
    "\n",
    "transition_surprisal = init_transition_surprisal_lookup(day1_data, 'path', 'prev_path', 'status', 'success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fail': 0,\n",
       " 'fsurprisal': 11.266786540694902,\n",
       " 'ssurprisal': 7.56634682255381,\n",
       " 'success': 13}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_transition_surprisal(path, prev_path, surprisal, data):\n",
    "    if path not in list(surprisal.keys()):\n",
    "        denum = len(data)\n",
    "        return {\n",
    "            'fail': 0,\n",
    "            'success': 0,\n",
    "            'ssurprisal': log(1/(1/denum),2),\n",
    "            'fsurprisal': log(1/(1/denum),2),\n",
    "        }\n",
    "    else:\n",
    "        if prev_path not in list(surprisal[path].keys()):\n",
    "            denum = len(data.loc[(data['path'] == path)])\n",
    "            return {\n",
    "                'fail': 0,\n",
    "                'success': 0,\n",
    "                'ssurprisal': log(1/(1/denum),2),\n",
    "                'fsurprisal': log(1/(1/denum),2),\n",
    "            }\n",
    "        else:\n",
    "            return surprisal[path][prev_path]\n",
    "\n",
    "get_transition_surprisal('buy_item', 'login', transition_surprisal, day1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'login': 5.004031754019051, 'view_item': 43.01743977398506, 'sell_item': 0, 'buy_item': 29.630434620642397, 'logout': 10.26912667914942, 'home': 0, 'payment_modify': 0, 'view_profile': 11.715961990255144, 'end': 10.96072599483906, 'update_address': 4.08746284125034, 'comment': 0, 'password_reset': 0, 'bank_modify': 0, 'update_email': 0}\n",
      "{'login': 5.004031754019051, 'view_item': 37.40227139284127, 'sell_item': 0, 'buy_item': 8.681824039973746, 'logout': 10.26912667914942, 'home': 0, 'payment_modify': 0, 'view_profile': 0, 'end': 10.96072599483906, 'update_address': 0, 'comment': 0, 'password_reset': 0, 'bank_modify': 0, 'update_email': 0}\n"
     ]
    }
   ],
   "source": [
    "def get_user_transition_score(data, surprisal, key, feature, success_val):\n",
    "    accumulator = {}\n",
    "    key_last_path = {}\n",
    "    \n",
    "    for index,row in data.iterrows():\n",
    "        if row[key] not in key_last_path.keys():\n",
    "            key_last_path[row[key]] = \"\"\n",
    "            \n",
    "        if row[key] not in accumulator.keys():\n",
    "            accumulator[row[key]] = {k:0 for k in data[feature].unique()}\n",
    "            \n",
    "        if row[feature] is success_val:\n",
    "            accumulator[row[key]][row[feature]] += get_transition_surprisal(row[feature],key_last_path[row[key]], surprisal, data)['ssurprisal']\n",
    "        else:\n",
    "            accumulator[row[key]][row[feature]] += get_transition_surprisal(row[feature],key_last_path[row[key]], surprisal, data)['fsurprisal']\n",
    "\n",
    "        key_last_path[row[key]] = row[feature]\n",
    "                                    \n",
    "    return accumulator\n",
    "\n",
    "\n",
    "user_transition_score = get_user_transition_score(day1_data, transition_surprisal, 'user', 'path', 'success')\n",
    "\n",
    "print(user_transition_score['fraudster96'])\n",
    "print(user_transition_score['buyer402'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go through the logs of the day, can we identify outliers?\n",
    "\n",
    "To do so, the idea is to look at each action being taken by each user, and add the relevant value from the surprisal lookup table.\n",
    "\n",
    "That said, I did read a lot about information theory and surprisal analysis, and this most probably not how it is supposed to be used, and the calculation is most probably wrong... but this mistake is quite useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cumulative_score = [[v,sum(user_transition_score[v].values())] for v in [k for k in list(user_transition_score.keys())]]\n",
    "\n",
    "df_cumulative_score = pd.DataFrame(cumulative_score, columns=['user', 'surprisal'])\n",
    "\n",
    "avg = df_cumulative_score['surprisal'].mean()\n",
    "std = df_cumulative_score['surprisal'].std()\n",
    "df_cumulative_score['z'] = (df_cumulative_score['surprisal'] - avg) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>surprisal</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>bot776</td>\n",
       "      <td>56075.901968</td>\n",
       "      <td>36.461053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>bot1705</td>\n",
       "      <td>38118.593485</td>\n",
       "      <td>24.748691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>bot900</td>\n",
       "      <td>7981.984783</td>\n",
       "      <td>5.092579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>bot672</td>\n",
       "      <td>5668.535410</td>\n",
       "      <td>3.583669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>bot1946</td>\n",
       "      <td>4831.875321</td>\n",
       "      <td>3.037971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user     surprisal          z\n",
       "561    bot776  56075.901968  36.461053\n",
       "425   bot1705  38118.593485  24.748691\n",
       "1365   bot900   7981.984783   5.092579\n",
       "1438   bot672   5668.535410   3.583669\n",
       "229   bot1946   4831.875321   3.037971"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cumulative_score.loc[df_cumulative_score['z'] >= 2].sort_values(by=['surprisal'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>surprisal</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>buyer528</td>\n",
       "      <td>19.945630</td>\n",
       "      <td>-0.100532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>merchant238</td>\n",
       "      <td>19.945630</td>\n",
       "      <td>-0.100532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>buyer414</td>\n",
       "      <td>19.424189</td>\n",
       "      <td>-0.100872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>attacker520</td>\n",
       "      <td>15.964758</td>\n",
       "      <td>-0.103128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>merchant449</td>\n",
       "      <td>10.960726</td>\n",
       "      <td>-0.106392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user  surprisal         z\n",
       "1996     buyer528  19.945630 -0.100532\n",
       "1998  merchant238  19.945630 -0.100532\n",
       "914      buyer414  19.424189 -0.100872\n",
       "958   attacker520  15.964758 -0.103128\n",
       "569   merchant449  10.960726 -0.106392"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cumulative_score.sort_values(by=['surprisal'], ascending=False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best 2 {True: {True: 0.8681318681318682, False: 0.10718358038768529}, False: {True: 0.13186813186813187, False: 0.8928164196123147}} 0.8681318681318682 0.13186813186813187 2\n",
      "best 3 {True: {True: 0.8681318681318682, False: 0.10718358038768529}, False: {True: 0.13186813186813187, False: 0.8928164196123147}} 0.8681318681318682 0.13186813186813187 3\n",
      "best 8 {True: {True: 0.875, False: 0.10206422018348624}, False: {True: 0.125, False: 0.8979357798165137}} 0.875 0.125 8\n",
      "limit 8\n",
      "count True {True: 146, False: 24}\n",
      "count False {True: 200, False: 1566}\n",
      "percent True {True: 0.8588235294117647, False: 0.11325028312570781}\n",
      "percent False {True: 0.1411764705882353, False: 0.8867497168742922}\n"
     ]
    }
   ],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore', over='ignore')\n",
    "\n",
    "if random_seed:\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "maxlimit = 1\n",
    "maxtp = 0\n",
    "minfp = 1\n",
    "best_flat_lookup = {}\n",
    "\n",
    "for l in range(2, 10):\n",
    "    flat_status, flat_lookup = cheat_calculate_hit_rate(day1_data, user_transition_score, l)\n",
    "    if maxtp <= flat_lookup[True][True] or minfp >= flat_lookup[False][True]:\n",
    "        maxtp = flat_lookup[True][True]\n",
    "        minfp = flat_lookup[False][True]\n",
    "        maxlimit = l\n",
    "        print('best', l, flat_lookup, maxtp, minfp, maxlimit)\n",
    "    \n",
    "\n",
    "flat_status, flat_lookup = cheat_calculate_hit_rate(day1_data, user_transition_score, maxlimit)\n",
    "\n",
    "print('limit', maxlimit)\n",
    "print('count', True, flat_status[True])\n",
    "print('count', False, flat_status[False])\n",
    "\n",
    "print('percent', True, flat_lookup[True])\n",
    "print('percent', False, flat_lookup[False])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we cluster users together?\n",
    "\n",
    "\n",
    "We will use two lists: \n",
    "- unclassified_users, which is a copy of the original user_score list, but not classified yet\n",
    "- behaviour_types: this is a list of hashes, the key being the user type (normally type + n), and the value of that hash is an array with all the profiles of the users classified under that type\n",
    "\n",
    "With these 2 lists, here is the idea:\n",
    "- From the unclassified_users list, we will pick one at random\n",
    "- Then we will pick 100 users at random, and try to find 10 that matches the behaviour. It's ok if we don't find any\n",
    "- I make the hypothesis that we have 10 kind of users, so my prior probability is 0.1\n",
    "- Then we check in all the keys under behaviour_types and compare our found random candidates with max 10 random candidates from that behaviour type. For the experimental data, We know that the True Positive probability is 51.5%, and the True Negative rate is 11.2%. By doing enough tests, we should be able to update our belief that we are comparing matching or different profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_probability(prior_probability, distribution, test_result):\n",
    "    \n",
    "    # What is our success rate for this test_result?\n",
    "    likelihood_of_being_right = distribution[test_result][True]\n",
    "    likelihood_of_being_wrong = distribution[test_result][False]\n",
    "          \n",
    "    numerator = likelihood_of_being_right * prior_probability\n",
    "    denominator = (likelihood_of_being_right * prior_probability) + (likelihood_of_being_wrong * (1 - prior_probability))\n",
    "    \n",
    "    posterior_probability = numerator / denominator\n",
    "    \n",
    "    return posterior_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'login': 5.004031754019051, 'view_item': 43.01743977398506, 'sell_item': 0, 'buy_item': 29.630434620642397, 'logout': 10.26912667914942, 'home': 0, 'payment_modify': 0, 'view_profile': 11.715961990255144, 'end': 10.96072599483906, 'update_address': 4.08746284125034, 'comment': 0, 'password_reset': 0, 'bank_modify': 0, 'update_email': 0}\n",
      "{'login': 5.004031754019051, 'view_item': 37.40227139284127, 'sell_item': 0, 'buy_item': 8.681824039973746, 'logout': 10.26912667914942, 'home': 0, 'payment_modify': 0, 'view_profile': 0, 'end': 10.96072599483906, 'update_address': 0, 'comment': 0, 'password_reset': 0, 'bank_modify': 0, 'update_email': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dklp': 15.854436058599713, 'dklq': -0.24392176319022993, 'test': False}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore', over='ignore')\n",
    "\n",
    "def compare_profiles(profile1, profile2, limit = 7):\n",
    "    u1 = np.array(list(profile1.values()))\n",
    "    u2 = np.array(list(profile2.values()))\n",
    "    \n",
    "    # Ref: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Definition\n",
    "    px = [1/np.power(2,x) for x in u1]    \n",
    "    qx = [1/np.power(2,x) for x in u2]\n",
    "    \n",
    "    p = np.array(qx)/np.array(px)\n",
    "    q = np.array(px)/np.array(qx)\n",
    "    dklp = (qx * np.log2(p)).sum()\n",
    "    dklq = (px * np.log2(q)).sum()\n",
    "    \n",
    "    t = dklp < limit and dklp >= -limit and dklq < limit and dklq >= -limit\n",
    "    \n",
    "    return {'test': t, 'dklp': dklp, 'dklq': dklq}\n",
    "\n",
    "print(user_transition_score['fraudster96'])\n",
    "print(user_transition_score['buyer402'])\n",
    "compare_profiles(user_transition_score['fraudster96'], user_transition_score['buyer402'], maxlimit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_from_classification(candidate_name, behaviour_type_table):\n",
    "    cleaneds = []\n",
    "    empties = []\n",
    "    for be, be_list in behaviour_type_table.items():\n",
    "        if candidate_name in behaviour_type_table[be]:\n",
    "            behaviour_type_table[be].remove(candidate_name)\n",
    "            cleaneds.append(be)\n",
    "        if len(behaviour_type_table[be]) == 0:\n",
    "            empties.append(be)\n",
    "    for e in empties:\n",
    "        del behaviour_type_table[e]\n",
    "            \n",
    "    return cleaneds           \n",
    "            \n",
    "\n",
    "def classify_candidates(candidate_name, behaviour_type_table, score, limit = 7):\n",
    "    potential_matching_type = {}\n",
    "    passing_score = 0.85\n",
    "    sample_size = 20\n",
    "    small_size_adjustment = 2\n",
    "    \n",
    "    for be, be_list in behaviour_type_table.items():\n",
    "        be_samples = random.sample(be_list, min(len(be_list), sample_size))\n",
    "\n",
    "        post = 0.1 # this is the prior\n",
    "        for idx in range(len(be_samples)):\n",
    "            y = be_samples[idx]\n",
    "            result = compare_profiles(score[candidate_name], score[y], limit)\n",
    "            post = update_probability(post, flat_lookup, result['test'])\n",
    "            \n",
    "        if post >= passing_score * (min(small_size_adjustment,max(1,len(be_samples)))/small_size_adjustment):\n",
    "            potential_matching_type[be] = post\n",
    "\n",
    "\n",
    "    if len(potential_matching_type) == 0:\n",
    "        \n",
    "        new_class_name = max(0,len(list(behaviour_type_table.values()))) + 1\n",
    "        return new_class_name\n",
    "    else:\n",
    "        return max(potential_matching_type, key=potential_matching_type.get)\n",
    "\n",
    "def add_candidate_to_behaviour_type(candidate_name, matching_class, behaviour_type_table):  \n",
    "    if matching_class not in behaviour_type_table.keys():\n",
    "        behaviour_type_table[matching_class] = []\n",
    "\n",
    "    if candidate_name not in behaviour_type_table[matching_class]:\n",
    "        behaviour_type_table[matching_class].append(candidate_name)\n",
    "        \n",
    "    return candidate_name\n",
    "    \n",
    "def classify_users_in_list(unclassified_user_lists, behaviour_type_table, score, limit = 7):\n",
    "    # select one user\n",
    "    candidate_name = random.choice(unclassified_user_lists)\n",
    "    if candidate_name:\n",
    "        # classify user\n",
    "        cleanup = remove_from_classification(candidate_name, behaviour_type_table)\n",
    "        matching_class = classify_candidates(candidate_name, behaviour_type_table, score, limit)\n",
    "\n",
    "        # add the user to the proper type\n",
    "        add_candidate_to_behaviour_type(candidate_name, matching_class, behaviour_type_table)\n",
    "        unclassified_user_lists.remove(candidate_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if random_seed:\n",
    "    random.seed(random_seed)\n",
    "\n",
    "behaviour_type_table = {}\n",
    "unclassified_user_lists = random.sample(list(user_transition_score.keys()), len(list(user_transition_score.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 83.0782877089 188 {'buyer': 188}\n",
      "2 90.1513520804 519 {'buyer': 519}\n",
      "3 130.977119884 502 {'merchant': 502}\n",
      "4 127.623825961 494 {'merchant': 494}\n",
      "5 165.965142825 178 {'buyer': 178}\n",
      "6 156.153730885 53 {'buyer': 53}\n",
      "7 50.320694373 2 {'fraudster': 2}\n",
      "8 21.817352416 3 {'buyer': 2, 'merchant': 1}\n",
      "9 226.493136815 4 {'spammer': 4}\n",
      "10 40.5556503097 9 {'attacker': 6, 'buyer': 1, 'fraudster': 2}\n",
      "11 43.5422474766 5 {'merchant': 5}\n",
      "12 86.2656941198 2 {'buyer': 2}\n",
      "13 26.233884428 1 {'fraudster': 1}\n",
      "14 46.2312074636 5 {'merchant': 5}\n",
      "15 27.2315442896 2 {'buyer': 2}\n",
      "16 140.976558557 2 {'fraudster': 1, 'buyer': 1}\n",
      "17 65.8488134826 1 {'merchant': 1}\n",
      "18 213.812498724 3 {'merchant': 3}\n",
      "19 4831.8753215 1 {'bot': 1}\n",
      "20 10.9607259948 1 {'merchant': 1}\n",
      "21 190.084814964 1 {'attacker': 1}\n",
      "22 5668.53541029 1 {'bot': 1}\n",
      "23 39.9211269787 4 {'buyer': 4}\n",
      "24 131.819810554 2 {'spammer': 2}\n",
      "25 169.848157892 1 {'buyer': 1}\n",
      "26 7981.98478332 1 {'bot': 1}\n",
      "27 127.794035183 2 {'buyer': 2}\n",
      "28 38118.5934849 1 {'bot': 1}\n",
      "29 221.282191908 2 {'buyer': 2}\n",
      "30 244.669084138 1 {'buyer': 1}\n",
      "31 155.567927532 3 {'merchant': 3}\n",
      "32 230.616114245 2 {'merchant': 2}\n",
      "33 246.91864566 2 {'buyer': 2}\n",
      "34 56075.9019678 1 {'bot': 1}\n",
      "CPU times: user 31.9 s, sys: 146 ms, total: 32 s\n",
      "Wall time: 32.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# while there are unclassified users\n",
    "while len(unclassified_user_lists[:10]):\n",
    "    classify_users_in_list(unclassified_user_lists, behaviour_type_table, user_transition_score, maxlimit)\n",
    "\n",
    "for k in behaviour_type_table.keys():\n",
    "    type_average = np.mean([sum(user_transition_score[x].values()) for x in behaviour_type_table[k]])\n",
    "    print(k, type_average, len(behaviour_type_table[k]), cheat_lookup_all_users(behaviour_type_table[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spammer150', 'spammer1935'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day1_data.loc[day1_data['user'].isin(behaviour_type_table[24])]['user'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'login': 5.004031754019051, 'view_item': 127.05185860243785, 'sell_item': 0, 'buy_item': 0, 'logout': 0, 'home': 0, 'payment_modify': 0, 'view_profile': 0, 'end': 10.96072599483906, 'update_address': 0, 'comment': 58.15837321097585, 'password_reset': 0, 'bank_modify': 0, 'update_email': 0}\n",
      "{'login': 5.004031754019051, 'view_item': 29.88319573653219, 'sell_item': 0, 'buy_item': 0, 'logout': 0, 'home': 0, 'payment_modify': 0, 'view_profile': 0, 'end': 10.96072599483906, 'update_address': 0, 'comment': 16.616678060278815, 'password_reset': 0, 'bank_modify': 0, 'update_email': 0}\n",
      "compare test {'test': True, 'dklp': 0.00041349357463677807, 'dklq': -1.2914255633106044e-16}\n"
     ]
    }
   ],
   "source": [
    "a = 'spammer150'\n",
    "b = 'spammer1935'\n",
    "\n",
    "print(user_transition_score[a])\n",
    "print(user_transition_score[b])\n",
    "print('compare test', compare_profiles(user_transition_score[a], user_transition_score[b],6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2, Let's see if we can find something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now Day 2. Surprisal values are based on a different day, but if the normal distribution is anything like Day 1, the surprisal value calculated should be still relevant.\n",
    "\n",
    "Let's generate a new day of logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = datetime(2019,1,2,0,0)\n",
    "number_of_new_users = 20\n",
    "existing_users = user_lists[:]\n",
    "\n",
    "if random_seed:\n",
    "    random.seed(random_seed + 1)\n",
    "\n",
    "user_list_day2s = generate_userlist(number_of_new_users, existing_users)\n",
    "\n",
    "if random_seed:\n",
    "    random.seed(random_seed + 1)\n",
    "day2_logs = generate_logs(user_list_day2s, start_time)\n",
    "\n",
    "print(len(day2_logs), 'logs events generated for', len(user_list_day2s), 'users')\n",
    "\n",
    "day2_data = transform_logs_to_pandas(day2_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_transition_score_day2 = get_user_transition_score(day2_data, transition_surprisal, 'user', 'path', 'success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we use our log stream analyser. This is basically a state machine that keep an ongoing total of each users. \n",
    "\n",
    "The theory is that if a user only do normal actions, the sum of the surprisal value of all his actions should be fairly low (under ~10... but this is arbitrary). Anything over 10 would mean that a high number of unlikely actions were performed. Lets see if we can identify which users stands out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cumulative_score = [[v,sum(user_transition_score_day2[v].values())] for v in [k for k in list(user_transition_score_day2.keys())]]\n",
    "\n",
    "df_cumulative_score = pd.DataFrame(cumulative_score, columns=['user', 'surprisal'])\n",
    "\n",
    "avg = df_cumulative_score['surprisal'].mean()\n",
    "std = df_cumulative_score['surprisal'].std()\n",
    "df_cumulative_score['z'] = (df_cumulative_score['surprisal'] - avg) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cumulative_score.loc[df_cumulative_score['z'] >= 2].sort_values(by=['surprisal'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if random_seed:\n",
    "    random.seed(random_seed)\n",
    "\n",
    "unclassified_user_lists = random.sample(list(user_transition_score_day2.keys()), len(list(user_transition_score_day2.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# while there are unclassified users\n",
    "while len(unclassified_user_lists):\n",
    "    classify_users_in_list(unclassified_user_lists, behaviour_type_table, user_transition_score_day2, maxlimit)\n",
    "\n",
    "for k in behaviour_type_table.keys():\n",
    "    type_average = np.mean([sum(user_transition_score[x].values()) for x in behaviour_type_table[k]])\n",
    "    print(k, type_average, len(behaviour_type_table[k]), cheat_lookup_all_users(behaviour_type_table[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why these users are identified as outliers?\n",
    "\n",
    "If we look at the Top 1 outlier, we can see each action performed, and the surprisal value of each action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at a normal user, we can see that the surprisal value assigned to each value is really low, only slightly contributing to give that user a high score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cumulative_score.sort_values(by=['surprisal'], ascending=False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Automatically Classifying Users\n",
    "\n",
    "At one point, we still need to classify behaviours. Using Naive Bayes over the actions, we can score each users to the likely category they belong too.\n",
    "\n",
    "Could we have done that without looking for the outliers? Yes, but we still need to identify which users is normal and which one is likely not.\n",
    "\n",
    "At first, we need to calculate the probability distribution of each actions for each categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this probability distribution over a log stream"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
